{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Bag of Words"
      ],
      "metadata": {
        "id": "ZRB3IBkD4G4U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQY9N3Ok3xi7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the datasets\n",
        "trainData = pd.read_csv('/content/trainData.csv').dropna(subset=['Body'])\n",
        "testData = pd.read_csv('/content/testData.csv').dropna(subset=['Body'])\n",
        "valData = pd.read_csv('/content/validationData.csv').dropna(subset=['Body'])\n",
        "\n",
        "# Setup the data\n",
        "X_train = trainData['Body']\n",
        "y_train = trainData['label']\n",
        "X_test = testData['Body']\n",
        "y_test = testData['label']\n",
        "X_val = valData['Body']\n",
        "y_val = valData['label']\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline_rf = Pipeline([\n",
        "    ('bow', CountVectorizer()),  # Changed from 'tfidf' to 'bow'\n",
        "    ('clf', RandomForestClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# Parameters to tune\n",
        "parameters_rf = {\n",
        "    'bow__max_df': (0.75, 0.85),\n",
        "    'bow__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
        "    'clf__n_estimators': [100, 300],  # number of trees in the forest\n",
        "    'clf__max_depth': [10, 20, None]  # maximum depth of the tree\n",
        "}\n",
        "\n",
        "# # Parameters to tune\n",
        "# parameters_rf = {\n",
        "#     'clf__n_estimators': [100,300,350,400],  # number of trees in the forest\n",
        "#     'clf__max_depth': [10,20,None]  # maximum depth of the tree\n",
        "# }\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning with StratifiedKFold Cross-Validation\n",
        "grid_search_rf = GridSearchCV(pipeline_rf, parameters_rf, cv=stratified_kfold, n_jobs=-1, verbose=1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "# Print best score and parameters\n",
        "print(\"Best score (Random Forest): %0.3f\" % grid_search_rf.best_score_)\n",
        "print(\"Best parameters set (Random Forest):\")\n",
        "best_parameters_rf = grid_search_rf.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_rf.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_rf[param_name]))\n",
        "\n",
        "# Use the best parameters to re-train the final model on the entire training data\n",
        "pipeline_rf.set_params(**best_parameters_rf)\n",
        "pipeline_rf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "predictions_val_rf = pipeline_rf.predict(X_val)\n",
        "val_accuracy = accuracy_score(y_val, predictions_val_rf)\n",
        "print(f'Validation Accuracy (Random Forest): {val_accuracy:.4f}')\n",
        "print(\"Confusion Matrix (Random Forest) - Validation:\")\n",
        "print(confusion_matrix(y_val, predictions_val_rf))\n",
        "print(\"Classification Report (Random Forest) - Validation:\")\n",
        "print(classification_report(y_val, predictions_val_rf))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "predictions_rf = pipeline_rf.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, predictions_rf)\n",
        "print(f'Test Accuracy (Random Forest): {test_accuracy:.4f}')\n",
        "print(\"Confusion Matrix (Random Forest) - Test:\")\n",
        "print(confusion_matrix(y_test, predictions_rf))\n",
        "print(\"Classification Report (Random Forest) - Test:\")\n",
        "print(classification_report(y_test, predictions_rf))\n",
        "\n",
        "# Calculate additional metrics\n",
        "y_test_proba = pipeline_rf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "auc = roc_auc_score(y_test, y_test_proba)\n",
        "f1 = f1_score(y_test, predictions_rf)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
        "tpf = tpr[1]  # True Positive Fraction (same as TPR for the positive class)\n",
        "fpf = fpr[1]  # False Positive Fraction (same as FPR for the positive class)\n",
        "\n",
        "print(f\"AUC: {auc:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Visualize the confusion matrix for test data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, predictions_rf), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Class 0', 'Predicted Class 1'],\n",
        "            yticklabels=['Actual Class 0', 'Actual Class 1'])\n",
        "plt.title('Confusion Matrix - Test Data')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Create scatter plot of AUC vs F1-Score\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(auc, f1, color='red')\n",
        "plt.title('Scatter Plot of AUC vs F1-Score')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TF-IDF"
      ],
      "metadata": {
        "id": "DXUo5NqB4qcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the datasets\n",
        "trainData = pd.read_csv('/content/trainData.csv').dropna(subset=['Body'])\n",
        "testData = pd.read_csv('/content/testData.csv').dropna(subset=['Body'])\n",
        "valData = pd.read_csv('/content/validationData.csv').dropna(subset=['Body'])\n",
        "\n",
        "# Setup the data\n",
        "X_train = trainData['Body']\n",
        "y_train = trainData['label']\n",
        "X_test = testData['Body']\n",
        "y_test = testData['label']\n",
        "X_val = valData['Body']\n",
        "y_val = valData['label']\n",
        "\n",
        "# Define the pipeline with TF-IDF\n",
        "pipeline_tfidf = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_df=0.7)),\n",
        "    ('clf', RandomForestClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# # Parameters to tune\n",
        "# parameters_tfidf = {\n",
        "#     'tfidf__max_df': (0.75, 0.85),\n",
        "#     'tfidf__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
        "#     'clf__n_estimators': [100, 200,300],  # number of trees in the forest\n",
        "#     'clf__oob_score': [True]\n",
        "#     'clf__max_depth': [10, 20, None]  # maximum depth of the tree\n",
        "# }\n",
        "# Parameters to tune\n",
        "parameters_tfidf = {\n",
        "    'tfidf__max_df': (0.75, 0.85),\n",
        "    'tfidf__ngram_range': [(1, 1)],  # unigrams or bigrams\n",
        "    'clf__n_estimators': [500],  # number of trees in the forest\n",
        "    'clf__oob_score': [True],\n",
        "    'clf__max_depth': [None],  # maximum depth of the tree\n",
        "    'clf__min_samples_leaf':[2]\n",
        "\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning with StratifiedKFold Cross-Validation\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search_tfidf = GridSearchCV(pipeline_tfidf, parameters_tfidf, cv=stratified_kfold, n_jobs=-1, verbose=2)\n",
        "grid_search_tfidf.fit(X_train, y_train)\n",
        "\n",
        "# Print best score and parameters\n",
        "print(\"Best score (TF-IDF + Random Forest): %0.3f\" % grid_search_tfidf.best_score_)\n",
        "print(\"Best parameters set (TF-IDF + Random Forest):\")\n",
        "best_parameters_tfidf = grid_search_tfidf.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_tfidf.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_tfidf[param_name]))\n",
        "\n",
        "# Use the best parameters to re-train the final model on the entire training data\n",
        "pipeline_tfidf.set_params(**best_parameters_tfidf)\n",
        "pipeline_tfidf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "predictions_val_tfidf = pipeline_tfidf.predict(X_val)\n",
        "val_accuracy_tfidf = accuracy_score(y_val, predictions_val_tfidf)\n",
        "print(f'Validation Accuracy (TF-IDF + Random Forest): {val_accuracy_tfidf:.4f}')\n",
        "print(\"Confusion Matrix (TF-IDF + Random Forest) - Validation:\")\n",
        "print(confusion_matrix(y_val, predictions_val_tfidf))\n",
        "print(\"Classification Report (TF-IDF + Random Forest) - Validation:\")\n",
        "print(classification_report(y_val, predictions_val_tfidf))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "predictions_tfidf = pipeline_tfidf.predict(X_test)\n",
        "test_accuracy_tfidf = accuracy_score(y_test, predictions_tfidf)\n",
        "print(f'Test Accuracy (TF-IDF + Random Forest): {test_accuracy_tfidf:.4f}')\n",
        "print(\"Confusion Matrix (TF-IDF + Random Forest) - Test:\")\n",
        "print(confusion_matrix(y_test, predictions_tfidf))\n",
        "print(\"Classification Report (TF-IDF + Random Forest) - Test:\")\n",
        "print(classification_report(y_test, predictions_tfidf))\n",
        "\n",
        "# Calculate additional metrics for TF-IDF\n",
        "y_test_proba_tfidf = pipeline_tfidf.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "auc_tfidf = roc_auc_score(y_test, y_test_proba_tfidf)\n",
        "f1_tfidf = f1_score(y_test, predictions_tfidf)\n",
        "fpr_tfidf, tpr_tfidf, thresholds_tfidf = roc_curve(y_test, y_test_proba_tfidf)\n",
        "tpf_tfidf = tpr_tfidf[1]  # True Positive Fraction (same as TPR for the positive class)\n",
        "fpf_tfidf = fpr_tfidf[1]  # False Positive Fraction (same as FPR for the positive class)\n",
        "\n",
        "print(f\"AUC (TF-IDF): {auc_tfidf:.4f}\")\n",
        "print(f\"F1-Score (TF-IDF): {f1_tfidf:.4f}\")\n",
        "\n",
        "# Visualize the confusion matrix for test data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, predictions_tfidf), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Class 0', 'Predicted Class 1'],\n",
        "            yticklabels=['Actual Class 0', 'Actual Class 1'])\n",
        "plt.title('Confusion Matrix - Test Data (TF-IDF)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yvzXUGbJ4tRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec"
      ],
      "metadata": {
        "id": "u1NNod8o45gX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, f1_score\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the datasets\n",
        "trainData = pd.read_csv('/content/trainData.csv').dropna(subset=['Body'])\n",
        "testData = pd.read_csv('/content/testData.csv').dropna(subset=['Body'])\n",
        "valData = pd.read_csv('/content/validationData.csv').dropna(subset=['Body'])\n",
        "\n",
        "# Setup the data\n",
        "X_train = trainData['Body']\n",
        "y_train = trainData['label']\n",
        "X_test = testData['Body']\n",
        "y_test = testData['label']\n",
        "X_val = valData['Body']\n",
        "y_val = valData['label']\n",
        "\n",
        "# Tokenize the messages for Word2Vec\n",
        "X_train_tokenized = X_train.apply(word_tokenize)\n",
        "X_val_tokenized = X_val.apply(word_tokenize)\n",
        "X_test_tokenized = X_test.apply(word_tokenize)\n",
        "\n",
        "# Train a Word2Vec model\n",
        "w2v_model = Word2Vec(sentences=X_train_tokenized, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Function to convert text to Word2Vec vectors\n",
        "def text_to_w2v(text, model):\n",
        "    words = word_tokenize(text)\n",
        "    words = [word for word in words if word in model.wv]\n",
        "    if len(words) == 0:\n",
        "        return np.zeros(model.vector_size)\n",
        "    return np.mean(model.wv[words], axis=0)\n",
        "\n",
        "# Custom transformer for Word2Vec\n",
        "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return np.array([text_to_w2v(text, self.model) for text in X])\n",
        "\n",
        "# Define the pipeline with Word2Vec\n",
        "pipeline_w2v = Pipeline([\n",
        "    ('w2v', Word2VecTransformer(w2v_model)),\n",
        "    ('clf', RandomForestClassifier(random_state=42)),\n",
        "])\n",
        "\n",
        "# Parameters to tune\n",
        "parameters_w2v = {\n",
        "    'clf__n_estimators': [100, 300],  # number of trees in the forest\n",
        "    'clf__max_depth': [10, 20, None]  # maximum depth of the tree\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning with StratifiedKFold Cross-Validation\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search_w2v = GridSearchCV(pipeline_w2v, parameters_w2v, cv=stratified_kfold, n_jobs=-1, verbose=1)\n",
        "grid_search_w2v.fit(X_train, y_train)\n",
        "\n",
        "# Print best score and parameters\n",
        "print(\"Best score (Word2Vec + Random Forest): %0.3f\" % grid_search_w2v.best_score_)\n",
        "print(\"Best parameters set (Word2Vec + Random Forest):\")\n",
        "best_parameters_w2v = grid_search_w2v.best_estimator_.get_params()\n",
        "for param_name in sorted(parameters_w2v.keys()):\n",
        "    print(\"\\t%s: %r\" % (param_name, best_parameters_w2v[param_name]))\n",
        "\n",
        "# Use the best parameters to re-train the final model on the entire training data\n",
        "pipeline_w2v.set_params(**best_parameters_w2v)\n",
        "pipeline_w2v.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model on the validation data\n",
        "predictions_val_w2v = pipeline_w2v.predict(X_val)\n",
        "val_accuracy_w2v = accuracy_score(y_val, predictions_val_w2v)\n",
        "print(f'Validation Accuracy (Word2Vec + Random Forest): {val_accuracy_w2v:.4f}')\n",
        "print(\"Confusion Matrix (Word2Vec + Random Forest) - Validation:\")\n",
        "print(confusion_matrix(y_val, predictions_val_w2v))\n",
        "print(\"Classification Report (Word2Vec + Random Forest) - Validation:\")\n",
        "print(classification_report(y_val, predictions_val_w2v))\n",
        "\n",
        "# Evaluate the model on the test data\n",
        "predictions_w2v = pipeline_w2v.predict(X_test)\n",
        "test_accuracy_w2v = accuracy_score(y_test, predictions_w2v)\n",
        "print(f'Test Accuracy (Word2Vec + Random Forest): {test_accuracy_w2v:.4f}')\n",
        "print(\"Confusion Matrix (Word2Vec + Random Forest) - Test:\")\n",
        "print(confusion_matrix(y_test, predictions_w2v))\n",
        "print(\"Classification Report (Word2Vec + Random Forest) - Test:\")\n",
        "print(classification_report(y_test, predictions_w2v))\n",
        "\n",
        "# Calculate additional metrics for Word2Vec\n",
        "y_test_proba_w2v = pipeline_w2v.predict_proba(X_test)[:, 1]  # Get probabilities for the positive class\n",
        "auc_w2v = roc_auc_score(y_test, y_test_proba_w2v)\n",
        "f1_w2v = f1_score(y_test, predictions_w2v)\n",
        "fpr_w2v, tpr_w2v, thresholds_w2v = roc_curve(y_test, y_test_proba_w2v)\n",
        "tpf_w2v = tpr_w2v[1]  # True Positive Fraction (same as TPR for the positive class)\n",
        "fpf_w2v = fpr_w2v[1]  # False Positive Fraction (same as FPR for the positive class)\n",
        "\n",
        "print(f\"AUC (Word2Vec): {auc_w2v:.4f}\")\n",
        "print(f\"F1-Score (Word2Vec): {f1_w2v:.4f}\")\n",
        "\n",
        "# Visualize the confusion matrix for test data\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix(y_test, predictions_w2v), annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted Class 0', 'Predicted Class 1'],\n",
        "            yticklabels=['Actual Class 0', 'Actual Class 1'])\n",
        "plt.title('Confusion Matrix - Test Data (Word2Vec)')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UmtLGNdH43g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scatter plot of AUC vs F1-Score for TFIDF and Word2Vec"
      ],
      "metadata": {
        "id": "cNTGtDd25ChZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create scatter plot of AUC vs F1-Score for both models\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(auc_tfidf, f1_tfidf, color='red', label='TF-IDF + Random Forest')\n",
        "plt.scatter(auc_w2v, f1_w2v, color='blue', label='Word2Vec + Random Forest')\n",
        "plt.title('Scatter Plot of AUC vs F1-Score')\n",
        "plt.xlabel('AUC')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Id2K6c1M5NUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}