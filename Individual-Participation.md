# Individual Participation

| Name             | Component                              | Contribution                                                                          |
| ---------------- | -------------------------------------- | ------------------------------------------------------------------------------------- |
| Paulina Gonzalez | ML Technique 2: SVM                    | * Started SVM model ipynb file. * Used TfidfVectorizer to transform the email body text into TF-IDF vectors, representing the importance of words. * Trained an SVM classifier using the TF-IDF vectors of the email bodies. * Experimented with different values of the regularization parameter C to prevent overfitting. * Evaluated the model’s performance on the test set using accuracy, confusion matrix, and classification report. * Displayed evaluation metrics to assess the effectiveness of the model in detecting phishing emails. * Generated word clouds for phishing and non-phishing emails to visualize the most common words. * Created confusion matrix plots to understand the performance of the model in terms of true positives, false positives, true negatives, and false negatives. * Plotted the ROC curve and calculated the AUC score to evaluate the model’s ability to distinguish between classes. * Applied cross-validation to validate the model and ensure it generalizes well to unseen data. * Calculated training and test accuracies to check for overfitting. * Generated learning curves to visualize the training and validation scores as a function of the number of training examples. |
| Jheel Shah       | Data Preprocessing                     | Initial Dataset Examination; Coordinating Preprocessing Start Points with teammates; Started looking for vectorizartion techniques; Started TF-IDF vectorization and model training ;Bag of Words vectorization and model training using the new updated training, validation and testing datasets; Provided teammates with the updated code for further implementation of ML models; Implemented word2vec vecorization and provided code to teammates|
| Jharna Kumari    | Data Augumentation                     | Uvic dataset and Kaggle Normal Email dataset Augment. Collaborate with teammate to work on the same on colab. Downsample normal emails dataset. Split data into training, validation and test sets before feature extraction. Explore about PCA for dimensionality reduction. Removed Stopwords from the dataset in the cleaning process. Supplied training, validation and testing datasets to teammates resulting in increased accuracy. Implemented PCA but that led to decreased accuracy for ML models. Explore how to calculate correct number of principal components. |
| Bilel Matmti     | ML Technique 1: Logistic Regression    | Study existing research on email-based phishing detection, exploring the dataset to understand its structure and typical features found in emails, researching how Logistic Regression works and how to implement it, setting up logistic regression base code on jupyter notebook, Implemented TFIDF with Logistic Regression, constant improvements to our model (created multiple versions of our code & used multiple versions of our preprocessed dataset to obtain the best results possible), feature engineering, constantly analyzing and comparing results of our performance metrics used in logistic regression, implementing optimizations to improve our model, working with Zixin to implement cross-validation, constant reporting of our results in README file for logistic regression that is systemically organized & updated as progression occurs. Implemented Word2Vec and Logistic Regression with cross-validation      |       
| Adel Agha        | ML Technique 3: Random Forest          | Random Forest Classifier Research, Optimization and tuning parameters(Depth and Nestimators) for best performance. Attempting and developed different feature extraction techniques like OneHot and TF-IDF Vectorization, body text and misspelled word count. Applied Hypertuning parameters using RandomizedSearchCV to Validation and Test Data. Attempted another Hyperparameter tuning function RandomizedSearchCV, but execution kept on crashing. Tried useful arguments/functions for imbalance dataset challenge such class_weight={0: 1, 1: 2}, giving the spam detection twice importance when predicting. Continuuos exploration of TFIDF vectorizer functions like ngram, a method for tokenizing text for better performance. Working with balncing the model for reducing overfitting and underfitting. Multi-Classifier System will be attempted.                        |
| Zixin Li         | ML Technique 1: Logistic Regression    | Implemented logistic regression optimization code to find the best threshold for classification.Utilized metrics such as F1 score, precision, recall, and balanced accuracy to evaluate model performance at different thresholds.Identified the best threshold that maximizes the F1 score, and evaluated the model using the best threshold, calculating the AUC score, confusion matrix, and balanced accuracy score.Analyzed the confusion matrix at the best threshold to understand the model's performance in terms of TP, TN, FP, and FN.Discussed the importance of balanced accuracy in evaluating the model on imbalanced datasets.Designed a plot to visualize the metrics versus thresholds, providing insights into how the model's performance varies with different decision thresholds.|
| Summer Liu       | ML Technique 2: SVM                    | Added cross-validation, compared the accuracy of two feature extraction methods.  Added roc curve graph and learning curves.|                    
| Birva Patel      | ML Technique 3: Random Forest          | Random Forest: Optimized and developed the Random Forest classifier using Scikit-Learn. Researched the algorithm and resolved ParserError issues with CSV files. Managed data preprocessing and incomplete file uploads. Developed hyperparameter tuning code, evaluated performance, and managed computational resources on Google Colab. Researched BERT for its potential applications.|                         
