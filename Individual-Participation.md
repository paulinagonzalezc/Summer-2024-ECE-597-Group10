# Individual Participation

| Name             | Component                              | Contribution                                                                          |
| ---------------- | -------------------------------------- | ------------------------------------------------------------------------------------- |
| Paulina Gonzalez | ML Technique 2: SVM                    | Added cross-validation, implemented learning curve of training and cross-validation to check for overfitting. Also, added regularization c parameter to reduce overfitting and change code to use new dataset. |
| Jheel Shah       | Data Preprocessing                     | Initial Dataset Examination; Coordinating Preprocessing Start Points with teammates; TF-IDF vectorization and model training; Bag of Words vectorization and model training using the new updated training, validation and testing datasets; Provided teammates with the updated code for further implementation of ML models|
| Jharna Kumari    | Data Augumentation                     | Uvic dataset and Kaggle Normal Email dataset Augment. Collaborate with teammate to work on the same on colab. Downsample normal emails dataset. Split data into training, validation and test sets before feature extraction. Explore about PCA for dimensionality reduction. Removed Stopwords from the dataset in the cleaning process. Supplied training, validation and testing datasets to teammates resulting in increased accuracy. Implemented PCA but that led to decreased accuracy for ML models. |
| Bilel Matmti     | ML Technique 1: Logistic Regression    | Logistic Regression Research, Setting up logistic regression on Jupyter Notebook, Coding and optimizations CONTINUED, Working on cross-validation and implementing optimizations     |       
| Adel Agha        | ML Technique 3: Random Forest          | Random Forest Classifier Research, Optimization and tuning parameters(Depth and Nestimators) for best performance. Attempting and developed different feature extraction techniques like OneHot and TF-IDF Vectorization, body text and misspelled word count. Applied Hypertuning parameters using RandomizedSearchCV to Validation and Test Data. Improved overall accuracy to 96% and F1 score. Attempted another Hyperparameter tuning function RandomizedSearchCV, but execution kept on crashing. Tried useful arguments/functions for imbalance dataset challenge such class_weight={0: 1, 1: 2}, giving the spam detection twice importance when predicting. Continuuos exploration of TFIDF vectorizer functions like ngram, a method for tokenizing text for better performance. Currently exploring SciLearn's FeatureHasher and DictVectorizer vectorizers for improving performance, and working with balncing the model for reducing overfitting and underfitting. Multi-Classifier System will be attempted.                        |
| Zixin Li         | ML Technique 1: Logistic Regression    | Logistic Regression Research, Logistic Regression Optimization, Logistic Regression Cross Validation                                                          |
| Summer Liu       | ML Technique 2: SVM                    | Added cross-validation, compared the accuracy of two feature extraction methods.  Added roc curve graph.|                    
| Birva Patel      | ML Technique 3: Random Forest          | Random Forest: Optimized and developed the Random Forest classifier using Scikit-Learn. Researched the algorithm and resolved ParserError issues with CSV files. Managed data preprocessing and incomplete file uploads. Developed hyperparameter tuning code, evaluated performance, and managed computational resources on Google Colab. Researched BERT for its potential applications.|                         
